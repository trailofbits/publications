# Incubated Machine Learning Exploits: Backdooring ML Pipelines Using Input-Handling Bugs

Machine learning (ML) pipelines are vulnerable to model backdoors that compromise the integrity of the underlying system. 
Although many backdoor attacks limit the attack surface to the model, ML models are not standalone objects. 
Instead, they are artifacts built using a wide range of tools and embedded into pipelines with many interacting components.

In this talk, we introduce incubated ML exploits in which attackers inject model backdoors into ML pipelines using input-handling bugs in ML tools. 
Using a language-theoretic security (LangSec) framework, we systematically exploited ML model serialization bugs in popular tools to construct backdoors. 
In the process, we developed malicious artifacts such as polyglot and ambiguous files using ML model files. 
We also contributed to Fickling, a pickle security tool tailored for ML use cases. 
Finally, we formulated a set of guidelines for security researchers and ML practitioners. 
By chaining system security issues and model vulnerabilities, incubated ML exploits emerge as a new class of exploits that highlight the importance of a holistic approach to ML security.

Presented at 
* [HOPE XV](https://livestream.com/accounts/9198012/events/11160133/videos/248463591)
* [BSides Las Vegas 2024](https://www.youtube.com/watch?v=CNCM57DzWqU&t=1002s)
* [DEFCON 32](https://www.youtube.com/watch?v=Z38pTFM0FyU&t=1s)

Resources 
* [Slides](DEFCON_32_Suha_Sabi_Hussain_Incubated_ML_Exploits.pdf)

Authored by 
* Suha Sabi Hussain
